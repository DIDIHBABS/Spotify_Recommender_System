{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:28:04.674821Z",
     "start_time": "2023-12-21T08:28:04.625938Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"spotify_millsongdata.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:54:39.191720Z",
     "start_time": "2023-12-21T08:54:36.610682Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "  artist                   song                                        link  \\\n0   ABBA  Ahe's My Kind Of Girl  /a/abba/ahes+my+kind+of+girl_20598417.html   \n1   ABBA       Andante, Andante       /a/abba/andante+andante_20002708.html   \n2   ABBA         As Good As New        /a/abba/as+good+as+new_20003033.html   \n3   ABBA                   Bang                  /a/abba/bang_20598415.html   \n4   ABBA       Bang-A-Boomerang      /a/abba/bang+a+boomerang_20002668.html   \n\n                                                text  \n0  Look at her face, it's a wonderful face  \\r\\nA...  \n1  Take it easy with me, please  \\r\\nTouch me gen...  \n2  I'll never know why I had to go  \\r\\nWhy I had...  \n3  Making somebody happy is a question of give an...  \n4  Making somebody happy is a question of give an...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>artist</th>\n      <th>song</th>\n      <th>link</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ABBA</td>\n      <td>Ahe's My Kind Of Girl</td>\n      <td>/a/abba/ahes+my+kind+of+girl_20598417.html</td>\n      <td>Look at her face, it's a wonderful face  \\r\\nA...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ABBA</td>\n      <td>Andante, Andante</td>\n      <td>/a/abba/andante+andante_20002708.html</td>\n      <td>Take it easy with me, please  \\r\\nTouch me gen...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ABBA</td>\n      <td>As Good As New</td>\n      <td>/a/abba/as+good+as+new_20003033.html</td>\n      <td>I'll never know why I had to go  \\r\\nWhy I had...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ABBA</td>\n      <td>Bang</td>\n      <td>/a/abba/bang_20598415.html</td>\n      <td>Making somebody happy is a question of give an...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ABBA</td>\n      <td>Bang-A-Boomerang</td>\n      <td>/a/abba/bang+a+boomerang_20002668.html</td>\n      <td>Making somebody happy is a question of give an...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T07:11:26.675968Z",
     "start_time": "2023-12-21T07:11:26.619475Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#Data Cleaning"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T07:12:16.784793Z",
     "start_time": "2023-12-21T07:12:16.695364Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57650 entries, 0 to 57649\n",
      "Data columns (total 4 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   artist  57650 non-null  object\n",
      " 1   song    57650 non-null  object\n",
      " 2   link    57650 non-null  object\n",
      " 3   text    57650 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 1.8+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T07:12:27.072251Z",
     "start_time": "2023-12-21T07:12:26.724039Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "artist    0\nsong      0\nlink      0\ntext      0\ndtype: int64"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T07:12:47.747926Z",
     "start_time": "2023-12-21T07:12:47.583276Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39mdrop(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlink\u001B[39m\u001B[38;5;124m'\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m      2\u001B[0m data \u001B[38;5;241m=\u001B[39m data\u001B[38;5;241m.\u001B[39msample(\u001B[38;5;241m10000\u001B[39m)\n\u001B[1;32m      3\u001B[0m data\u001B[38;5;241m.\u001B[39mreset_index(drop\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data = data.drop('link', axis=1)\n",
    "data = data.sample(10000)\n",
    "data.reset_index(drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T09:47:35.720479Z",
     "start_time": "2023-12-21T09:47:35.449910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "#Regex"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T07:25:02.728008Z",
     "start_time": "2023-12-21T07:25:02.693972Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [],
   "source": [
    "data['text'] = data['text'].str.lower()\n",
    "data['text'] = data['text'].replace(r'^\\w\\s\\r', ' ').replace(r'\\n', ' ', regex = True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:54:56.150240Z",
     "start_time": "2023-12-21T08:54:55.736041Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Tokenization and Stemming"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [],
   "source": [
    "#stemmer = PorterStemmer()\n",
    "def tokenize(txt):\n",
    "    tokens = word_tokenize(txt)\n",
    "    lemma = [nltk.WordNetLemmatizer(i) for i in tokens]\n",
    "    #stemming = [stemmer.stem(w) for w in tokens]\n",
    "    return \" \".join(lemma)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:54:59.807117Z",
     "start_time": "2023-12-21T08:54:59.416666Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/PY3/english.pickle\u001B[0m\n\n  Searched in:\n    - '/Users/babs/nltk_data'\n    - '/Users/babs/anaconda3/nltk_data'\n    - '/Users/babs/anaconda3/share/nltk_data'\n    - '/Users/babs/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mLookupError\u001B[0m                               Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[86], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: tokenize(x))\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/series.py:4771\u001B[0m, in \u001B[0;36mSeries.apply\u001B[0;34m(self, func, convert_dtype, args, **kwargs)\u001B[0m\n\u001B[1;32m   4661\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mapply\u001B[39m(\n\u001B[1;32m   4662\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m   4663\u001B[0m     func: AggFuncType,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4666\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs,\n\u001B[1;32m   4667\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m DataFrame \u001B[38;5;241m|\u001B[39m Series:\n\u001B[1;32m   4668\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   4669\u001B[0m \u001B[38;5;124;03m    Invoke function on values of Series.\u001B[39;00m\n\u001B[1;32m   4670\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   4769\u001B[0m \u001B[38;5;124;03m    dtype: float64\u001B[39;00m\n\u001B[1;32m   4770\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m-> 4771\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m SeriesApply(\u001B[38;5;28mself\u001B[39m, func, convert_dtype, args, kwargs)\u001B[38;5;241m.\u001B[39mapply()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1123\u001B[0m, in \u001B[0;36mSeriesApply.apply\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_str()\n\u001B[1;32m   1122\u001B[0m \u001B[38;5;66;03m# self.f is Callable\u001B[39;00m\n\u001B[0;32m-> 1123\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mapply_standard()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/core/apply.py:1174\u001B[0m, in \u001B[0;36mSeriesApply.apply_standard\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1172\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1173\u001B[0m         values \u001B[38;5;241m=\u001B[39m obj\u001B[38;5;241m.\u001B[39mastype(\u001B[38;5;28mobject\u001B[39m)\u001B[38;5;241m.\u001B[39m_values\n\u001B[0;32m-> 1174\u001B[0m         mapped \u001B[38;5;241m=\u001B[39m lib\u001B[38;5;241m.\u001B[39mmap_infer(\n\u001B[1;32m   1175\u001B[0m             values,\n\u001B[1;32m   1176\u001B[0m             f,\n\u001B[1;32m   1177\u001B[0m             convert\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconvert_dtype,\n\u001B[1;32m   1178\u001B[0m         )\n\u001B[1;32m   1180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(mapped) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(mapped[\u001B[38;5;241m0\u001B[39m], ABCSeries):\n\u001B[1;32m   1181\u001B[0m     \u001B[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001B[39;00m\n\u001B[1;32m   1182\u001B[0m     \u001B[38;5;66;03m#  See also GH#25959 regarding EA support\u001B[39;00m\n\u001B[1;32m   1183\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m obj\u001B[38;5;241m.\u001B[39m_constructor_expanddim(\u001B[38;5;28mlist\u001B[39m(mapped), index\u001B[38;5;241m=\u001B[39mobj\u001B[38;5;241m.\u001B[39mindex)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2924\u001B[0m, in \u001B[0;36mpandas._libs.lib.map_infer\u001B[0;34m()\u001B[0m\n",
      "Cell \u001B[0;32mIn[86], line 1\u001B[0m, in \u001B[0;36m<lambda>\u001B[0;34m(x)\u001B[0m\n\u001B[0;32m----> 1\u001B[0m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtext\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;28;01mlambda\u001B[39;00m x: tokenize(x))\n",
      "Cell \u001B[0;32mIn[85], line 3\u001B[0m, in \u001B[0;36mtokenize\u001B[0;34m(txt)\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtokenize\u001B[39m(txt):\n\u001B[0;32m----> 3\u001B[0m     tokens \u001B[38;5;241m=\u001B[39m word_tokenize(txt)\n\u001B[1;32m      4\u001B[0m     lemma \u001B[38;5;241m=\u001B[39m [nltk\u001B[38;5;241m.\u001B[39mWordNetLemmatizer(i) \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m tokens]\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;66;03m#stemming = [stemmer.stem(w) for w in tokens]\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:129\u001B[0m, in \u001B[0;36mword_tokenize\u001B[0;34m(text, language, preserve_line)\u001B[0m\n\u001B[1;32m    114\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mword_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m, preserve_line\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[1;32m    115\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    116\u001B[0m \u001B[38;5;124;03m    Return a tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended word tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    127\u001B[0m \u001B[38;5;124;03m    :type preserve_line: bool\u001B[39;00m\n\u001B[1;32m    128\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 129\u001B[0m     sentences \u001B[38;5;241m=\u001B[39m [text] \u001B[38;5;28;01mif\u001B[39;00m preserve_line \u001B[38;5;28;01melse\u001B[39;00m sent_tokenize(text, language)\n\u001B[1;32m    130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    131\u001B[0m         token \u001B[38;5;28;01mfor\u001B[39;00m sent \u001B[38;5;129;01min\u001B[39;00m sentences \u001B[38;5;28;01mfor\u001B[39;00m token \u001B[38;5;129;01min\u001B[39;00m _treebank_word_tokenizer\u001B[38;5;241m.\u001B[39mtokenize(sent)\n\u001B[1;32m    132\u001B[0m     ]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/tokenize/__init__.py:106\u001B[0m, in \u001B[0;36msent_tokenize\u001B[0;34m(text, language)\u001B[0m\n\u001B[1;32m     96\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21msent_tokenize\u001B[39m(text, language\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124menglish\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n\u001B[1;32m     97\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     98\u001B[0m \u001B[38;5;124;03m    Return a sentence-tokenized copy of *text*,\u001B[39;00m\n\u001B[1;32m     99\u001B[0m \u001B[38;5;124;03m    using NLTK's recommended sentence tokenizer\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    104\u001B[0m \u001B[38;5;124;03m    :param language: the model name in the Punkt corpus\u001B[39;00m\n\u001B[1;32m    105\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 106\u001B[0m     tokenizer \u001B[38;5;241m=\u001B[39m load(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtokenizers/punkt/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlanguage\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.pickle\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mtokenize(text)\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/data.py:750\u001B[0m, in \u001B[0;36mload\u001B[0;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001B[0m\n\u001B[1;32m    747\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<<Loading \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mresource_url\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m>>\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    749\u001B[0m \u001B[38;5;66;03m# Load the resource.\u001B[39;00m\n\u001B[0;32m--> 750\u001B[0m opened_resource \u001B[38;5;241m=\u001B[39m _open(resource_url)\n\u001B[1;32m    752\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mformat\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraw\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    753\u001B[0m     resource_val \u001B[38;5;241m=\u001B[39m opened_resource\u001B[38;5;241m.\u001B[39mread()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/data.py:876\u001B[0m, in \u001B[0;36m_open\u001B[0;34m(resource_url)\u001B[0m\n\u001B[1;32m    873\u001B[0m protocol, path_ \u001B[38;5;241m=\u001B[39m split_resource_url(resource_url)\n\u001B[1;32m    875\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m protocol \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnltk\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m--> 876\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m find(path_, path \u001B[38;5;241m+\u001B[39m [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mopen()\n\u001B[1;32m    877\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m protocol\u001B[38;5;241m.\u001B[39mlower() \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfile\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    878\u001B[0m     \u001B[38;5;66;03m# urllib might not use mode='rb', so handle this one ourselves:\u001B[39;00m\n\u001B[1;32m    879\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m find(path_, [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m])\u001B[38;5;241m.\u001B[39mopen()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.11/site-packages/nltk/data.py:583\u001B[0m, in \u001B[0;36mfind\u001B[0;34m(resource_name, paths)\u001B[0m\n\u001B[1;32m    581\u001B[0m sep \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m*\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m70\u001B[39m\n\u001B[1;32m    582\u001B[0m resource_not_found \u001B[38;5;241m=\u001B[39m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mmsg\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00msep\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 583\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mLookupError\u001B[39;00m(resource_not_found)\n",
      "\u001B[0;31mLookupError\u001B[0m: \n**********************************************************************\n  Resource \u001B[93mpunkt\u001B[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001B[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001B[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001B[93mtokenizers/punkt/PY3/english.pickle\u001B[0m\n\n  Searched in:\n    - '/Users/babs/nltk_data'\n    - '/Users/babs/anaconda3/nltk_data'\n    - '/Users/babs/anaconda3/share/nltk_data'\n    - '/Users/babs/anaconda3/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "data['text'] = data['text'].apply(lambda x: tokenize(x))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:55:06.116428Z",
     "start_time": "2023-12-21T08:55:05.773212Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tfidvector = TfidfVectorizer(analyzer='word',stop_words='english')\n",
    "matrix = tfidvector.fit_transform(data['text'])\n",
    "similarity = cosine_similarity(matrix)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-12-21T08:55:20.783642Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "data": {
      "text/plain": "21901"
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[data['song'] == 'Bang'].index[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:53:38.944449Z",
     "start_time": "2023-12-21T08:53:37.988038Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "def recommendation(song_df):\n",
    "    idx = data[data['song'] == song_df].index[0]\n",
    "    distances = sorted(list(enumerate(similarity[idx])),reverse=True,key=lambda x:x[1])\n",
    "\n",
    "    songs = []\n",
    "    for m_id in distances[1:21]:\n",
    "        songs.append(data.iloc[m_id[0]].song)\n",
    "\n",
    "    return songs\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:11:59.069900Z",
     "start_time": "2023-12-21T08:11:58.880807Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "data": {
      "text/plain": "['Bang A Drum',\n 'Bang Bang',\n 'Bang Bang',\n 'Bang Bang',\n 'Bang Bang Bang',\n 'Bang Bang Bang',\n 'Pop That Thang',\n 'Big Bang Boogie',\n 'Things Meant To Be',\n 'Dust 2...',\n 'I Never Meant To Hurt You',\n 'Moon Is Blue',\n 'Bang',\n \"I'll Find My Way Home\",\n 'Only Tears',\n 'Vodevil',\n 'Act Of Love',\n 'Handful Of Dust',\n 'Paris St. Germain',\n 'You Were Meant For Me']"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recommendation('Crying Over You')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:27:41.521306Z",
     "start_time": "2023-12-21T08:27:41.460376Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(similarity,open('similarity.pkl','wb'))\n",
    "pickle.dump(data,open('df.pkl','wb'))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-21T08:27:30.688579Z",
     "start_time": "2023-12-21T08:27:28.533189Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
